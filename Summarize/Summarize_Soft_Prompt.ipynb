{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer,GPT2Model, GPT2Config, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and digits\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "# Define a function to tokenize, convert text to indices, and pad sequences\n",
    "def tokenize_and_pad(data_list, max_article_length=1021, max_highlights_length=1024):\n",
    "    tokenized_data_list = []\n",
    "    for article, highlights in data_list:\n",
    "        # Tokenize and convert to indices\n",
    "        article_tokens = tokenizer.encode(article, add_special_tokens=True)\n",
    "        highlights_tokens = tokenizer.encode(highlights, add_special_tokens=True)\n",
    "\n",
    "        # Pad sequences to specified lengths\n",
    "        padded_article_tokens = torch.tensor(article_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_article_length - len(article_tokens)))\n",
    "        padded_highlights_tokens = torch.tensor(highlights_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_highlights_length - len(highlights_tokens)))\n",
    "\n",
    "        # Append to the tokenized_data_list only if both token lists are not empty\n",
    "        if len(article_tokens) > 0 and len(highlights_tokens) > 0:\n",
    "            tokenized_data_list.append((padded_article_tokens, padded_highlights_tokens))\n",
    "\n",
    "    return tokenized_data_list\n",
    "\n",
    "\n",
    "def calculate_bleu_score(machine_results, reference_texts):\n",
    "    bleu_score = corpus_bleu([[ref.split()] for ref in reference_texts], [gen.split() for gen in machine_results])\n",
    "    return bleu_score\n",
    "\n",
    "def calculate_rouge_scores(generated_answers, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    total_rouge1, total_rouge2, total_rougeL = 0, 0, 0\n",
    "    for gen, ref in zip(generated_answers, ground_truth):\n",
    "        scores = scorer.score(gen, ref)\n",
    "        total_rouge1 += scores['rouge1'].fmeasure\n",
    "        total_rouge2 += scores['rouge2'].fmeasure\n",
    "        total_rougeL += scores['rougeL'].fmeasure\n",
    "    average_rouge1 = total_rouge1 / len(generated_answers)\n",
    "    average_rouge2 = total_rouge2 / len(generated_answers)\n",
    "    average_rougeL = total_rougeL / len(generated_answers)\n",
    "    return average_rouge1, average_rouge2, average_rougeL\n",
    "\n",
    "def calculate_bert_score(generated_answers, ground_truth):\n",
    "    scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "    P, R, F1 = scorer.score(generated_answers, ground_truth)\n",
    "    avg_precision = sum(p.mean() for p in P) / len(P)\n",
    "    avg_recall = sum(r.mean() for r in R) / len(R)\n",
    "    avg_f1 = sum(f1.mean() for f1 in F1) / len(F1)\n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'test.csv'\n",
    "\n",
    "# List to store tuples of (article, highlights)\n",
    "test_data_list = []\n",
    "\n",
    "# Read CSV file and extract relevant columns\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    for row in csv_reader:\n",
    "        article = row.get('article', '')\n",
    "        highlights = row.get('highlights', '')\n",
    "        test_data_list.append((article, highlights))\n",
    "        \n",
    "\n",
    "\n",
    "csv_file_path = 'train.csv'\n",
    "\n",
    "# List to store tuples of (article, highlights)\n",
    "train_data_list = []\n",
    "\n",
    "# Read CSV file and extract relevant columns\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    for row in csv_reader:\n",
    "        article = row.get('article', '')\n",
    "        highlights = row.get('highlights', '')\n",
    "        train_data_list.append((article, highlights))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "csv_file_path = 'validation.csv'\n",
    "\n",
    "# List to store tuples of (article, highlights)\n",
    "val_data_list = []\n",
    "\n",
    "# Read CSV file and extract relevant columns\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    for row in csv_reader:\n",
    "        article = row.get('article', '')\n",
    "        highlights = row.get('highlights', '')\n",
    "        val_data_list.append((article, highlights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "random.seed(14)  # You can choose any seed value\n",
    "\n",
    "# Calculate .% of the original data size\n",
    "one_percent_size = int(0.01 * len(test_data_list))\n",
    "\n",
    "# Randomly sample 1% of the data\n",
    "onetest_data_list = random.sample(test_data_list, one_percent_size)\n",
    "# Calculate 1% of the original data size\n",
    "one_percent_size = int(0.01 * len(train_data_list))\n",
    "\n",
    "# Randomly sample 1% of the data\n",
    "onetrain_data_list = random.sample(train_data_list, one_percent_size)\n",
    "# Calculate 1% of the original data size\n",
    "one_percent_size = int(0.01 * len(val_data_list))\n",
    "\n",
    "# Randomly sample 1% of the data\n",
    "oneval_data_list = random.sample(val_data_list, one_percent_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to your data\n",
    "ptrain_data_list = [(preprocess_text(article), preprocess_text(highlights)) for article, highlights in onetrain_data_list]\n",
    "ptest_data_list = [(preprocess_text(article), preprocess_text(highlights)) for article, highlights in onetest_data_list]\n",
    "pval_data_list = [(preprocess_text(article), preprocess_text(highlights)) for article, highlights in oneval_data_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1114 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define a pad token and add it to the tokenizer\n",
    "pad_token = tokenizer.eos_token\n",
    "tokenizer.add_tokens([pad_token])\n",
    "\n",
    "# Apply tokenization and padding to your datasets\n",
    "max_article_length = 1021\n",
    "max_highlights_length = 1024\n",
    "tokenized_train_data_list = tokenize_and_pad(ptrain_data_list, max_article_length, max_highlights_length)\n",
    "tokenized_test_data_list = tokenize_and_pad(ptest_data_list, max_article_length, max_highlights_length)\n",
    "tokenized_val_data_list = tokenize_and_pad(pval_data_list, max_article_length, max_highlights_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING TRAIN AND TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store input and target ids\n",
    "input_ids_train = []\n",
    "target_ids_train = []\n",
    "\n",
    "# Specify maximum lengths\n",
    "max_article_length = 1021\n",
    "max_highlights_length = 1024\n",
    "\n",
    "# Iterate through the tokenized data\n",
    "for padded_article_tokens, padded_highlights_tokens in tokenized_train_data_list:\n",
    "    # Truncate article tokens if greater than max_article_length\n",
    "    truncated_article_tokens = padded_article_tokens[:max_article_length]\n",
    "\n",
    "    # Truncate highlights tokens if greater than max_highlights_length\n",
    "    truncated_highlights_tokens = padded_highlights_tokens[:max_highlights_length]\n",
    "\n",
    "    # Append truncated article tokens to input_ids_train\n",
    "    input_ids_train.append(truncated_article_tokens)\n",
    "\n",
    "    # Append truncated highlights tokens to target_ids_train\n",
    "    target_ids_train.append(truncated_highlights_tokens)\n",
    "\n",
    "# Convert the lists to PyTorch tensors\n",
    "input_ids_train = torch.stack(input_ids_train)\n",
    "target_ids_train = torch.stack(target_ids_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store input and target ids\n",
    "input_ids_val = []\n",
    "target_ids_val = []\n",
    "\n",
    "# Specify maximum lengths\n",
    "max_article_length = 1021\n",
    "max_highlights_length = 1024\n",
    "\n",
    "# Iterate through the tokenized data\n",
    "for padded_article_tokens, padded_highlights_tokens in tokenized_val_data_list:\n",
    "    # Truncate article tokens if greater than max_article_length\n",
    "    truncated_article_tokens = padded_article_tokens[:max_article_length]\n",
    "\n",
    "    # Truncate highlights tokens if greater than max_highlights_length\n",
    "    truncated_highlights_tokens = padded_highlights_tokens[:max_highlights_length]\n",
    "\n",
    "    # Append truncated article tokens to input_ids_train\n",
    "    input_ids_val.append(truncated_article_tokens)\n",
    "\n",
    "    # Append truncated highlights tokens to target_ids_train\n",
    "    target_ids_val.append(truncated_highlights_tokens)\n",
    "\n",
    "# Convert the lists to PyTorch tensors\n",
    "input_ids_val = torch.stack(input_ids_val)\n",
    "target_ids_val = torch.stack(target_ids_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(input_ids_train.shape)\n",
    "# print(input_ids_train)\n",
    "\n",
    "# print(target_ids_train.shape)\n",
    "# print(target_ids_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0004, -0.1206,  0.0394,  ...,  0.2581, -0.1128,  0.0265],\n",
       "        [ 0.1030,  0.0311,  0.0340,  ..., -0.0216, -0.0657, -0.2662],\n",
       "        [ 0.0113,  0.1548, -0.0212,  ...,  0.1057,  0.2224, -0.0694]],\n",
       "       grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Define the number of prompts and embedding size\n",
    "num_prompts_token = 3  # \"summarize the following text\"\n",
    "embedding_size = 768\n",
    "\n",
    "# Define a specific sentence\n",
    "sentence = \"summarize\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "\n",
    "# Get the embeddings for the input_ids from the GPT-2 model\n",
    "gpt2_embeddings = gpt2_model.transformer.wte(input_ids)\n",
    "\n",
    "# Create an embedding layer for soft prompts and initialize with the sentence embeddings\n",
    "soft_prompt_embeddings = nn.Embedding(num_prompts_token, embedding_size)\n",
    "soft_prompt_embeddings.weight.data.copy_(gpt2_embeddings.squeeze(0))\n",
    "# print(\"Shape of soft prompt embeddings:\", soft_prompt_embeddings.weight.data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate soft prompt embeddings at the beginning of the input sequence\n",
    "class GPT2WithPromptTuning(nn.Module):\n",
    "    def __init__(self, gpt2_model, soft_prompt_embeddings):\n",
    "        super(GPT2WithPromptTuning, self).__init__()\n",
    "        self.gpt2_model = gpt2_model\n",
    "        self.soft_prompt_embeddings = soft_prompt_embeddings\n",
    "    \n",
    "    def forward(self, input_ids, soft_prompt_ids):\n",
    "        # Get the embeddings for the input_ids from the GPT-2 model\n",
    "        gpt2_embeddings = self.gpt2_model.transformer.wte(input_ids)\n",
    "        # Get the embeddings for the soft prompts\n",
    "        soft_prompt_embeds = self.soft_prompt_embeddings(soft_prompt_ids)\n",
    "         # Concatenate the embeddings\n",
    "        # print(\"Shape of soft prompt embeddings:\", soft_prompt_embeds.shape)\n",
    "        # print(\"soft prompt embeddings:\", soft_prompt_embeds)\n",
    "\n",
    "        # print(\"Shape of input embeddings:\", gpt2_embeddings.shape)\n",
    "        # print(\"soft input prompt embeddings:\", gpt2_embeddings)\n",
    "        \n",
    "        # Concatenate the embeddings\n",
    "        embeddings = torch.cat([soft_prompt_embeds, gpt2_embeddings], dim=0)\n",
    "\n",
    "        # print(\"Shape of concatenated embeddings:\", embeddings.shape)\n",
    "        # print(\"soft of concatenated embeddings:\", embeddings)\n",
    "        \n",
    "        # Pass the concatenated embeddings through the GPT-2 model\n",
    "        outputs = self.gpt2_model(inputs_embeds=embeddings)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(target_ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = GPT2WithPromptTuning(gpt2_model, soft_prompt_embeddings)\n",
    "\n",
    "# Freeze GPT-2 model weights\n",
    "for param in model.gpt2_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 8\n",
    "epochs = 1\n",
    "learning_rate = 2e-3\n",
    "gradient_clip_value = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.soft_prompt_embeddings.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "soft_prompt_ids = torch.tensor([0, 1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lists to store scores\n",
    "train_bleu_scores = []\n",
    "train_bert_scores = []\n",
    "train_rouge1_scores = []\n",
    "train_rouge2_scores = []\n",
    "train_rougeL_scores = []\n",
    "\n",
    "val_bleu_scores = []\n",
    "val_bert_scores = []\n",
    "val_rouge1_scores = []\n",
    "val_rouge2_scores = []\n",
    "val_rougeL_scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                                                             | 0/2871 [00:00<?, ?it/s, loss=11]/home/anshul/.virtualenvs/DS_app/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/anshul/.virtualenvs/DS_app/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/anshul/.virtualenvs/DS_app/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2871/2871 [46:01<00:00,  1.04it/s, loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 train_loss : 0.33796393871307373 val_loss : 0.4136776049110226\n",
      "Validation BLEU Score: 3.6364572685947105e-234\n",
      "Validation BERTScore: tensor(0.9556)\n",
      "Validation ROUGE-1 Score: 0.9656530383680721\n",
      "Validation ROUGE-2 Score: 0.9655081208871659\n",
      "Validation ROUGE-L Score: 0.9656456123957339\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Create a tqdm progress bar for the training data\n",
    "    data_iterator = tqdm(zip(input_ids_train, target_ids_train), desc=f'Epoch {epoch + 1}', total=len(input_ids_train))\n",
    "    \n",
    "    for input_ids, target_ids in data_iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move input and target tensors to GPU\n",
    "        input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "        # print(\"input\",input_ids.shape,\"target\",target_ids.shape)\n",
    "        # print(\"input\",len(input_ids),\"target\",len(target_ids))\n",
    "        \n",
    "        # Assuming you have a soft_prompt_ids for each training instance\n",
    "        # If not, you might need to modify this part accordingly\n",
    "        outputs = model(input_ids, soft_prompt_ids.to(device))\n",
    "        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n",
    "\n",
    "        loss = criterion(logits, target_ids)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_value)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar description with the current loss\n",
    "        data_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Convert tensor predictions and references to lists\n",
    "        predictions = logits.argmax(dim=-1).squeeze(0).tolist()\n",
    "        references = target_ids.squeeze(0).tolist()\n",
    "\n",
    "        # Calculate BLEU Score for training\n",
    "        bleu_score = calculate_bleu_score([tokenizer.decode(predictions)], [tokenizer.decode(references)])\n",
    "        train_bleu_scores.append(bleu_score)\n",
    "\n",
    "        # Calculate BERTScore for training\n",
    "        bert_precision, bert_recall, bert_f1 = calculate_bert_score([tokenizer.decode(predictions)], [tokenizer.decode(references)])\n",
    "        train_bert_scores.append(bert_f1)\n",
    "\n",
    "        # Calculate ROUGE Scores for training\n",
    "        rouge1, rouge2, rougeL = calculate_rouge_scores([tokenizer.decode(predictions)], [tokenizer.decode(references)])\n",
    "        train_rouge1_scores.append(rouge1)\n",
    "        train_rouge2_scores.append(rouge2)\n",
    "        train_rougeL_scores.append(rougeL)\n",
    "\n",
    "    # Calculate average training loss\n",
    "    # avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "\n",
    "#     # Validation loop\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_bleu_scores_epoch = []\n",
    "    val_bert_scores_epoch = []\n",
    "    val_rouge1_scores_epoch = []\n",
    "    val_rouge2_scores_epoch = []\n",
    "    val_rougeL_scores_epoch = []\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids_val, target_ids_val in zip(input_ids_val, target_ids_val):\n",
    "            input_ids_val, target_ids_val = input_ids_val.to(device), target_ids_val.to(device)\n",
    "            outputs_val = model(input_ids_val, soft_prompt_ids.to(device))\n",
    "            logits_val = outputs_val.logits if hasattr(outputs_val, \"logits\") else outputs_val.last_hidden_state\n",
    "            loss_val = criterion(logits_val, target_ids_val)\n",
    "            val_losses.append(loss_val.item())\n",
    "            # Convert tensor predictions and references to lists\n",
    "            predictions_val = logits_val.argmax(dim=-1).squeeze(0).tolist()\n",
    "            references_val = target_ids_val.squeeze(0).tolist()\n",
    "\n",
    "            # Calculate BLEU Score for validation\n",
    "            bleu_score_val = calculate_bleu_score([tokenizer.decode(predictions_val)], [tokenizer.decode(references_val)])\n",
    "            val_bleu_scores_epoch.append(bleu_score_val)\n",
    "\n",
    "            # Calculate BERTScore for validation\n",
    "            bert_precision_val, bert_recall_val, bert_f1_val = calculate_bert_score([tokenizer.decode(predictions_val)], [tokenizer.decode(references_val)])\n",
    "            val_bert_scores_epoch.append(bert_f1_val)\n",
    "\n",
    "            # Calculate ROUGE Scores for validation\n",
    "            rouge1_val, rouge2_val, rougeL_val = calculate_rouge_scores([tokenizer.decode(predictions_val)], [tokenizer.decode(references_val)])\n",
    "            val_rouge1_scores_epoch.append(rouge1_val)\n",
    "            val_rouge2_scores_epoch.append(rouge2_val)\n",
    "            val_rougeL_scores_epoch.append(rougeL_val)\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    print(\"epoch :\", epoch + 1,\"train_loss :\", loss.item(),\"val_loss :\", avg_val_loss)\n",
    "\n",
    "    # Calculate average validation scores\n",
    "    avg_bleu_score_val = sum(val_bleu_scores_epoch) / len(val_bleu_scores_epoch)\n",
    "    avg_bert_score_val = sum(val_bert_scores_epoch) / len(val_bert_scores_epoch)\n",
    "    avg_rouge1_score_val = sum(val_rouge1_scores_epoch) / len(val_rouge1_scores_epoch)\n",
    "    avg_rouge2_score_val = sum(val_rouge2_scores_epoch) / len(val_rouge2_scores_epoch)\n",
    "    avg_rougeL_score_val = sum(val_rougeL_scores_epoch) / len(val_rougeL_scores_epoch)\n",
    "\n",
    "    print(\"Validation BLEU Score:\", avg_bleu_score_val)\n",
    "    print(\"Validation BERTScore:\", avg_bert_score_val)\n",
    "    print(\"Validation ROUGE-1 Score:\", avg_rouge1_score_val)\n",
    "    print(\"Validation ROUGE-2 Score:\", avg_rouge2_score_val)\n",
    "    print(\"Validation ROUGE-L Score:\", avg_rougeL_score_val)\n",
    "\n",
    "    # Append validation scores\n",
    "    val_bleu_scores.append(avg_bleu_score_val)\n",
    "    val_bert_scores.append(avg_bert_score_val)\n",
    "    val_rouge1_scores.append(avg_rouge1_score_val)\n",
    "    val_rouge2_scores.append(avg_rouge2_score_val)\n",
    "    val_rougeL_scores.append(avg_rougeL_score_val)\n",
    "\n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "\n",
    "# Close the tqdm progress bar\n",
    "data_iterator.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training BLEU Score: 4.444515154690828e-05\n",
      "Average Training BERTScore: tensor(0.9407)\n",
      "Average Training ROUGE-1 Score: 0.9312832265021607\n",
      "Average Training ROUGE-2 Score: 0.9302407680675859\n",
      "Average Training ROUGE-L Score: 0.9310653933720154\n",
      "Average Validation BLEU Score: 3.6364572685947105e-234\n",
      "Average Validation BERTScore: tensor(0.9556)\n",
      "Average Validation ROUGE-1 Score: 0.9656530383680721\n",
      "Average Validation ROUGE-2 Score: 0.9655081208871659\n",
      "Average Validation ROUGE-L Score: 0.9656456123957339\n"
     ]
    }
   ],
   "source": [
    "# Calculate average scores for training\n",
    "avg_train_bleu_score = sum(train_bleu_scores) / len(train_bleu_scores)\n",
    "avg_train_bert_score = sum(train_bert_scores) / len(train_bert_scores)\n",
    "avg_train_rouge1_score = sum(train_rouge1_scores) / len(train_rouge1_scores)\n",
    "avg_train_rouge2_score = sum(train_rouge2_scores) / len(train_rouge2_scores)\n",
    "avg_train_rougeL_score = sum(train_rougeL_scores) / len(train_rougeL_scores)\n",
    "\n",
    "print(\"Average Training BLEU Score:\", avg_train_bleu_score)\n",
    "print(\"Average Training BERTScore:\", avg_train_bert_score)\n",
    "print(\"Average Training ROUGE-1 Score:\", avg_train_rouge1_score)\n",
    "print(\"Average Training ROUGE-2 Score:\", avg_train_rouge2_score)\n",
    "print(\"Average Training ROUGE-L Score:\", avg_train_rougeL_score)\n",
    "\n",
    "# Calculate average scores for validation\n",
    "avg_val_bleu_score = sum(val_bleu_scores) / len(val_bleu_scores)\n",
    "avg_val_bert_score = sum(val_bert_scores) / len(val_bert_scores)\n",
    "avg_val_rouge1_score = sum(val_rouge1_scores) / len(val_rouge1_scores)\n",
    "avg_val_rouge2_score = sum(val_rouge2_scores) / len(val_rouge2_scores)\n",
    "avg_val_rougeL_score = sum(val_rougeL_scores) / len(val_rougeL_scores)\n",
    "\n",
    "print(\"Average Validation BLEU Score:\", avg_val_bleu_score)\n",
    "print(\"Average Validation BERTScore:\", avg_val_bert_score)\n",
    "print(\"Average Validation ROUGE-1 Score:\", avg_val_rouge1_score)\n",
    "print(\"Average Validation ROUGE-2 Score:\", avg_val_rouge2_score)\n",
    "print(\"Average Validation ROUGE-L Score:\", avg_val_rougeL_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save model weights\n",
    "torch.save(model.state_dict(), 'Summarize_weights1.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1654566,
     "sourceId": 2734496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30579,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
